apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ .Release.Name }}-scripts
data:
  upload.py: |
    import paramiko
    import os
    transport = paramiko.Transport((os.environ['SFTP_HOST'], 22))
    transport.connect(username=os.environ['SFTP_USER'], password=os.environ['SFTP_PASS'])
    sftp = paramiko.SFTPClient.from_transport(transport)
    sftp.put('/tmp/test.txt', '/upload/test.txt')
    print("Uploaded")
    sftp.close()
  mock.py: |
    import paramiko
    import os
    import time
    import random
    import json
    from datetime import datetime
    
    file_types = ['trigger', 'scheduler', 'user']
    
    while True:
        transport = paramiko.Transport((os.environ['SFTP_HOST'], 22))
        transport.connect(username=os.environ['SFTP_USER'], password=os.environ['SFTP_PASS'])
        sftp = paramiko.SFTPClient.from_transport(transport)
        
        file_type = random.choice(file_types)
        timestamp = datetime.utcnow().strftime('%Y%m%d_%H%M%S')
        filename = f"{file_type}_{timestamp}_{random.randint(1000,9999)}.json"
        
        metadata = {
            'type': file_type,
            'created': datetime.utcnow().isoformat(),
            'data': f'demo {file_type} content'
        }
        
        with sftp.open(f'/upload/{filename}', 'w') as f:
            f.write(json.dumps(metadata).encode())
        
        sftp.close()
        print(f"Generated {filename} ({file_type})")
        time.sleep(1800)
  archive.py: |
    import paramiko
    import os
    import json
    from datetime import datetime, timedelta
    from google.cloud import storage
    
    # Archive thresholds in days
    thresholds = {
        'trigger': int(os.environ.get('TRIGGER_DAYS', '5')),
        'scheduler': int(os.environ.get('SCHEDULER_DAYS', '14')),
        'user': int(os.environ.get('USER_DAYS', '30'))
    }
    
    transport = paramiko.Transport((os.environ['SFTP_HOST'], 22))
    transport.connect(username=os.environ['SFTP_USER'], password=os.environ['SFTP_PASS'])
    sftp = paramiko.SFTPClient.from_transport(transport)
    
    files = sftp.listdir('/upload')
    client = storage.Client()
    bucket = client.bucket(os.environ['GCS_BUCKET'])
    
    for filename in files:
        if '_' in filename:
            parts = filename.split('_')
            if len(parts) >= 3:
                file_type = parts[0]
                try:
                    file_time = datetime.strptime(parts[1] + parts[2], '%Y%m%d%H%M%S')
                    age_days = (datetime.utcnow() - file_time).days
                    threshold = thresholds.get(file_type, 30)
                    
                    if age_days >= threshold:
                        sftp.get(f'/upload/{filename}', f'/tmp/{filename}')
                        blob = bucket.blob(f'archive/{file_type}/{filename}')
                        blob.upload_from_filename(f'/tmp/{filename}')
                        sftp.remove(f'/upload/{filename}')
                        print(f"Archived {filename} (age: {age_days}d >= {threshold}d)")
                except Exception as e:
                    print(f"Skip {filename}: {e}")
    sftp.close()
