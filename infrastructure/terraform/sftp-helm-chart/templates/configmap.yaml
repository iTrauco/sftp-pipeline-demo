apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ .Release.Name }}-scripts
data:
  upload.py: |
    import paramiko
    import os
    transport = paramiko.Transport((os.environ['SFTP_HOST'], 22))
    transport.connect(username=os.environ['SFTP_USER'], password=os.environ['SFTP_PASS'])
    sftp = paramiko.SFTPClient.from_transport(transport)
    sftp.put('/tmp/test.txt', '/upload/test.txt')
    print("Uploaded")
    sftp.close()
  mock.py: |
    import paramiko
    import os
    import time
    import random
    import json
    from datetime import datetime
    
    file_types = ['trigger', 'scheduler', 'user']
    
    while True:
        transport = paramiko.Transport((os.environ['SFTP_HOST'], 22))
        transport.connect(username=os.environ['SFTP_USER'], password=os.environ['SFTP_PASS'])
        sftp = paramiko.SFTPClient.from_transport(transport)
        
        file_type = random.choice(file_types)
        timestamp = datetime.utcnow().strftime('%Y%m%d_%H%M%S')
        filename = f"{file_type}_{timestamp}_{random.randint(1000,9999)}.json"
        
        metadata = {
            'type': file_type,
            'created': datetime.utcnow().isoformat(),
            'age_target': '24h',
            'data': f'demo {file_type} content'
        }
        
        with sftp.open(f'/upload/{filename}', 'w') as f:
            f.write(json.dumps(metadata).encode())
        
        sftp.close()
        print(f"Generated {filename} ({file_type})")
        time.sleep(1800)  # 30 minutes
  archive.py: |
    import paramiko
    import os
    import json
    from datetime import datetime, timedelta
    from google.cloud import storage
    
    transport = paramiko.Transport((os.environ['SFTP_HOST'], 22))
    transport.connect(username=os.environ['SFTP_USER'], password=os.environ['SFTP_PASS'])
    sftp = paramiko.SFTPClient.from_transport(transport)
    
    files = sftp.listdir('/upload')
    client = storage.Client()
    bucket = client.bucket(os.environ['GCS_BUCKET'])
    
    for filename in files:
        # Check file age from filename timestamp
        if '_' in filename:
            parts = filename.split('_')
            if len(parts) >= 3:
                file_date = parts[1] + parts[2]
                # Archive files older than 24 hours
                try:
                    file_time = datetime.strptime(parts[1], '%Y%m%d')
                    if datetime.utcnow() - file_time > timedelta(days=1):
                        sftp.get(f'/upload/{filename}', f'/tmp/{filename}')
                        blob = bucket.blob(f'archive/{filename}')
                        blob.upload_from_filename(f'/tmp/{filename}')
                        sftp.remove(f'/upload/{filename}')
                        print(f"Archived {filename}")
                except:
                    pass
    sftp.close()
